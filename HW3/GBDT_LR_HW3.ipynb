{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GBDT_LR_HW3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1SzmhnLjgM8"
      },
      "source": [
        "* [架構](https://i.imgur.com/umeIrJr.png)\n",
        "* [AUC](https://i.imgur.com/cXD1WYu.png)\n",
        "* [優缺點1](https://i.imgur.com/wiQKU5W.png)\n",
        "* [優缺點2](https://i.imgur.com/1UaPqK3.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_HYT036irRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4551cfa-cb0b-4f5c-a372-0445a8937de5"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error \n",
        "from sklearn.model_selection import KFold \n",
        "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
        "from sklearn.linear_model.logistic import LogisticRegression\n",
        "from sklearn.preprocessing.data import OneHotEncoder"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.gradient_boosting module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.logistic module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.preprocessing.data module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.preprocessing. Anything that cannot be imported from sklearn.preprocessing is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8L8qQ8mVMZR"
      },
      "source": [
        "user_item_path = '/content/drive/MyDrive/python_data/社群網路與推薦系統/hw3/data/Movielens/user_movie.dat'\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeOHfkmhQmLm"
      },
      "source": [
        "# Feature\n",
        "\n",
        "* X: [n_interaction, n_feature]：one hot for user, item, multi-hot for user_feature & item_feature\n",
        "* [GBDT for cross-feature](https://i.imgur.com/0deDZsX.png) \n",
        "  \n",
        "    GBDT應用概念：將X(Feature)作為input，利用GBDT產生對應的cross-feautre(樹中每個節點都是單個feauture)，利用GBDT中boosting的概念，強化對於錯誤的學習，找出更能代表data的cross-feature，將所有DT的結果(每個都產出對於error的預測)做one hot concat後得到最終embedding，再利用LR(Logistic Regression)進行分類預測，所以GBDT扮演Pretrain的角色。\n",
        "* n_CrossFeature = n_trees\n",
        "* 減少data imbalance：negative dowan sampling \n",
        "    \n",
        "    問題：\n",
        "    1. 使否將原始feature與cross-feauture做concat？\n",
        "    2. 決定n_tree, tree depth\n",
        "    3. 將power law & non power law feature各自分開train(各配GBDT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfJjMD9Fjlod"
      },
      "source": [
        "def get_feature(path):\n",
        "  names = ['id', 'feature_id']\n",
        "  df = pd.read_csv(path, sep= '\\t', names= names)\n",
        "  n = int(df['id'].max())\n",
        "  n_feature = int(df['feature_id'].max())\n",
        "  feature_mat = np.zeros(shape= (n, n_feature), dtype= float)\n",
        "  for i, row in df.iterrows():\n",
        "    feature_mat[int(row['id'])-1, int(row['feature_id'])-1] = 1 \n",
        "  return feature_mat"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGaPopYkVaEz"
      },
      "source": [
        "item_feature_mats = []\n",
        "user_feature_mats = []\n",
        "folder = '/content/drive/MyDrive/python_data/社群網路與推薦系統/hw3/data/Movielens/'\n",
        "for file in ['movie_genre', 'movie_movie(knn)']:\n",
        "  path = folder + file + '.dat'\n",
        "  item_feature_mat = get_feature(path= path)\n",
        "  item_feature_mats.append(item_feature_mat)\n",
        "for file in ['user_age', 'user_occupation']:\n",
        "  path = folder + file + '.dat'\n",
        "  user_feature_mat = get_feature(path= path)\n",
        "  user_feature_mats.append(user_feature_mat)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkPSuu8DVajX",
        "outputId": "30677ca1-06e5-4e70-d006-7f2766244c7c"
      },
      "source": [
        "item_feature_mat = np.concatenate(item_feature_mats, axis= 1)\n",
        "user_feature_mat = np.concatenate(user_feature_mats, axis= 1)\n",
        "print(f'item feature mat: {item_feature_mat.shape}')\n",
        "print(f'user feature mat: {user_feature_mat.shape}')\n",
        "n_user = user_feature_mat.shape[0]\n",
        "n_item = item_feature_mat.shape[0]\n",
        "d = n_item + item_feature_mat.shape[1] + n_user + user_feature_mat.shape[1]\n",
        "print(f'd: {d}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "item feature mat: (1682, 19)\n",
            "user feature mat: (943, 29)\n",
            "d: 2673\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIWsUMnmValW",
        "outputId": "dc7e49de-c80e-45c3-b260-3da57d690f76"
      },
      "source": [
        "rows = []\n",
        "y= []\n",
        "with open(user_item_path, 'r') as f:\n",
        "  for line in f.readlines():\n",
        "    user_temp = np.zeros(shape= (1, n_user), dtype= float)\n",
        "    item_temp = np.zeros(shape= (1, n_item), dtype= float)\n",
        "    user_id, item_id, rating, _= line.strip().split('\\t')\n",
        "    user_temp[0,int(user_id)-1] = 1\n",
        "    item_temp[0,int(item_id)-1] = 1\n",
        "    row = np.concatenate([user_temp, item_temp,  np.expand_dims(user_feature_mat[int(user_id)-1], axis= 0), np.expand_dims(item_feature_mat[int(item_id)-1], axis= 0)], axis= 1)\n",
        "    rows.append(row)\n",
        "    y.append(int(rating))\n",
        "\n",
        "X = np.concatenate(rows, axis= 0)\n",
        "y = np.array(y)\n",
        "print(f'X shape: {X.shape}')\n",
        "print(f'y shape: {y.shape}')\n",
        "print(f'n_rating: {len(set(y))}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape: (100000, 2673)\n",
            "y shape: (100000,)\n",
            "n_rating: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFp-7TotVjy7"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2nSvrpKVjA5"
      },
      "source": [
        "class GBDT_LR():\n",
        "  def __init__(self, x_train, y_train, x_test, y_test, n_estimator, depth, max_iter):\n",
        "    self.x_train = x_train \n",
        "    self.y_train = y_train\n",
        "    self.x_test = x_test \n",
        "    self.y_test = y_test\n",
        "    self.n_estimator = n_estimator\n",
        "    self.depth = depth \n",
        "    self.max_iter = max_iter\n",
        "  \n",
        "  def GBDT(self):\n",
        "    gbdt = GradientBoostingClassifier(n_estimators= self.n_estimator, max_depth= self.depth)\n",
        "    gbdt.fit(self.x_train, self.y_train)\n",
        "    \n",
        "    \"\"\"X：{array-like, sparse matrix} of shape (n_samples, n_features)\"\"\"\n",
        "    output = gbdt.apply(X= self.x_train) # Shape: [n_interaction, n_estimator, n_class]\n",
        "\n",
        "    \"\"\"One Hot Encoding\"\"\"\n",
        "    encoder = OneHotEncoder().fit(output[:, :, 0])\n",
        "    embedding = encoder.transform(output[:, :, 0])\n",
        "    return gbdt, encoder, embedding\n",
        "\n",
        "  def LR(self, enc_x, y):\n",
        "    lr = LogisticRegression(max_iter= self.max_iter)\n",
        "    lr.fit(enc_x, y)\n",
        "    return lr\n",
        "\n",
        "  def train(self):\n",
        "    gbdt, encoder, enc_x = self.GBDT()\n",
        "    lr = self.LR(enc_x= enc_x, y= self.y_train)\n",
        "    return gbdt, encoder, lr \n",
        "\n",
        "  def test(self):\n",
        "    gbdt, encoder, lr = self.train()\n",
        "    gbdt_output= gbdt.apply(X= self.x_test)\n",
        "    # encoder = OneHotEncoder().fit(gbdt_output[:, :, 0])\n",
        "    embedding = encoder.transform(gbdt_output[:, :, 0]) \n",
        "    pro = lr.predict_proba(embedding)\n",
        "    return pro\n",
        "    \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nk13T86gVNe"
      },
      "source": [
        "# Training Stage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Eow4CipgYCE"
      },
      "source": [
        "kf = KFold(n_splits=5)\n",
        "RMSEs = []\n",
        "n_estimator= 20\n",
        "depth= 5\n",
        "max_iter = 500"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38MZQKBTgfkA"
      },
      "source": [
        "for train_indices, test_indices in kf.split(X):\n",
        "  x_train, y_train = X[train_indices], y[train_indices]\n",
        "  x_test, y_test = X[test_indices], y[test_indices]\n",
        "  \n",
        "  model = GBDT_LR(x_train= x_train, y_train= y_train, x_test=x_test, y_test= y_test, n_estimator=n_estimator, depth=depth, max_iter= max_iter)\n",
        "  test_prob = model.test()\n",
        "  test_rating = np.argmax(test_prob, axis=1)+1\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uvf76rxfhtlO"
      },
      "source": [
        "print(test_prob.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgF7GfcJm-0Y"
      },
      "source": [
        "# np.argmax(test_prob, axis=1)+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYnLNnM73KUd"
      },
      "source": [
        "test_rating"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6pe6naGPAD6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}