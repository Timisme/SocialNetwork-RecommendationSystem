{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GBDT_LR_HW3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1SzmhnLjgM8"
      },
      "source": [
        "* [架構](https://i.imgur.com/umeIrJr.png)\n",
        "* [AUC](https://i.imgur.com/cXD1WYu.png)\n",
        "* [優缺點1](https://i.imgur.com/wiQKU5W.png)\n",
        "* [優缺點2](https://i.imgur.com/1UaPqK3.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_HYT036irRN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeOHfkmhQmLm"
      },
      "source": [
        "# Feature\n",
        "\n",
        "* X: [n_interaction, n_feature]：one hot for user, item, multi-hot for user_feature & item_feature\n",
        "* [GBDT for cross-feature](https://i.imgur.com/0deDZsX.png) \n",
        "  \n",
        "    GBDT應用概念：將X(Feature)作為input，利用GBDT產生對應的cross-feautre(樹中每個節點都是單個feauture)，利用GBDT中boosting的概念，強化對於錯誤的學習，找出更能代表data的cross-feature，將所有DT的結果(每個都產出對於error的預測)做one hot concat後得到最終embedding，再利用LR(Logistic Regression)進行分類預測，所以GBDT扮演Pretrain的角色。\n",
        "* n_CrossFeature = n_trees\n",
        "* 減少data imbalance：negative dowan sampling "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfJjMD9Fjlod"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}